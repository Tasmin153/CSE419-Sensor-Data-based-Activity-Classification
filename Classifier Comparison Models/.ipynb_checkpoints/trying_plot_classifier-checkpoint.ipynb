{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "\n",
    "h = .02  # step size in the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_finder(path):\n",
    "\n",
    "    file_list = []\n",
    "\n",
    "    for j in glob(path+\"*.csv\"):\n",
    "        file_list.append(j)\n",
    "\n",
    "    return file_list\n",
    "\n",
    "def read_file(path):\n",
    "\n",
    "    data = pd.read_csv(path) \n",
    "    return data\n",
    "\n",
    "def pd_to_np(data):\n",
    "    \n",
    "    if type(data) == np.ndarray:\n",
    "      print('Data is already in numpy format!')\n",
    "    else:\n",
    "      data = data.values\n",
    "      #print('Pandas to Numpy done!')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def string_to_index(activity_label):\n",
    "\n",
    "    activity_class =[]\n",
    "    har_class = {\n",
    "                    'Cook':0,\n",
    "                    'Eat':1,\n",
    "                    'Phone':2,\n",
    "                    'Read':3,\n",
    "                    'Watch_TV':4\n",
    "                }\n",
    "    for label in activity_label:\n",
    "        activity_class.append(har_class[label[0]])\n",
    "\n",
    "    return activity_class\n",
    "\n",
    "\n",
    "def data_loader(path, split=0.3):\n",
    "    X_train =  X_test =  y_train =  y_test = []\n",
    "    x = y = []\n",
    "    feature_list = []\n",
    "    \n",
    "    pd_data = read_file(path)\n",
    "\n",
    "    for i in pd_data:\n",
    "        feature_list.append(i)\n",
    "\n",
    "    selectData = pd_data.loc[:, feature_list[:-1]]\n",
    "    activityLabel = pd_data.loc[:, ['activity']]\n",
    "    x = pd_to_np(selectData)\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    y = string_to_index(activityLabel.values)\n",
    "    y = np.asarray(y) \n",
    "    y = y.astype('int32')\n",
    "    \n",
    "    #return x,y\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = split,random_state=42)\n",
    "    #return X_train, X_test, y_train, y_test\n",
    "    return x,y\n",
    "    \n",
    "# def full_dataset(file_list):\n",
    "\n",
    "#     X_train =  X_test =  y_train =  y_test = np.asarray([])\n",
    "#     X_train_temp =  X_test_temp =  y_train_temp =  y_test_temp = []\n",
    "\n",
    "#     for i in range(len(file_list)):\n",
    "#         X_train_temp, X_test_temp, y_train_temp, y_test_temp = data_loader(file_list[i])\n",
    "\n",
    "#         if i == 0:\n",
    "#             X_train = X_train_temp\n",
    "#             X_test  = X_test_temp\n",
    "#             y_train = y_train_temp\n",
    "#             y_test  = y_test_temp\n",
    "#         else:\n",
    "#             X_train = np.concatenate([X_train,X_train_temp],axis=0)\n",
    "#             X_test  = np.concatenate([X_test ,X_test_temp],axis=0)\n",
    "#             y_train = np.concatenate([y_train,y_train_temp],axis=0)\n",
    "#             y_test  = np.concatenate([y_test ,y_test_temp],axis=0)\n",
    "#     # print(X_train_temp.shape)\n",
    "#     # print(X_test_temp.shape)\n",
    "#     # print(y_train_temp.shape)\n",
    "#     # print(y_test_temp.shape) \n",
    "\n",
    "#     #return X_train, X_test, y_train, y_test\n",
    "#     return x,y\n",
    "\n",
    "def full_dataset(file_list):\n",
    "\n",
    "    x=y = np.asarray([])\n",
    "    x_temp =  y_temp =  []\n",
    "\n",
    "    for i in range(len(file_list)):\n",
    "        x_temp, y_temp = data_loader(file_list[i])\n",
    "\n",
    "        if i == 0:\n",
    "            x = x_temp\n",
    "            y = y_temp\n",
    "        else:\n",
    "            x = np.concatenate([x, x_temp],axis=0)\n",
    "            y  = np.concatenate([y ,y_temp],axis=0)\n",
    "    # print(X_train_temp.shape)\n",
    "    # print(X_test_temp.shape)\n",
    "    # print(y_train_temp.shape)\n",
    "    # print(y_test_temp.shape) \n",
    "\n",
    "    #return X_train, X_test, y_train, y_test\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
    "\n",
    "def train_model(model, X_train, y_train):\n",
    "    return model.fit(X_train, y_train)\n",
    "\n",
    "def test_model(model,X_test):\n",
    "\tpred = model.predict(X_test)\n",
    "\treturn pred\n",
    "    \n",
    "def model_init():\n",
    "\n",
    "\tmodel = classifiers[0]\n",
    "\t# model = DecisionTreeClassifier()\n",
    "\treturn model\n",
    "    \n",
    "    \n",
    "def heatmap_cm(confusion_matrix):\n",
    "    sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# For model evolution\n",
    "def model_evalution(y_test, y_pred):\n",
    "     #print(\"------------------- Model evaluation ----------------\\n\\n\")\n",
    "     print(\"Confusion Matrix : \\n\",confusion_matrix(y_test, y_pred))\n",
    "     print(\"--------------------------------------------\")\n",
    "     print(\"Accuracy Score : \",accuracy_score(y_test,y_pred))\n",
    "     #print(\"Classification Report : \\n\",classification_report(y_test, y_pred))\n",
    "     print(\"--------------------------------------------\")\n",
    "     print(\"\")\n",
    "    \n",
    "     heatmap_cm(pd.DataFrame(confusion_matrix(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Total files: 0\n"
     ]
    }
   ],
   "source": [
    "file_list = folder_finder(\"../Dataset/dataset_\")\n",
    "print(file_list)\n",
    "print('Total files:',len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y= full_dataset(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bf9367f67401>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# for i in range(len(file_list)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_arch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#pickle.dump(model, open(save_model_name, 'wb'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "x_test_bulk = []\n",
    "y_test_bulk = []\n",
    "model_arch = model_init()\n",
    "\n",
    "# for i in range(len(file_list)):\n",
    "for i in range(1,2):\n",
    "    X,y = data_loader(file_list[i])\n",
    "    model = train_model(model_arch, X,y)\n",
    "    #pickle.dump(model, open(save_model_name, 'wb'))\n",
    "    #x_test_bulk.append(X_test)\n",
    "    #y_test_bulk.append(y_test)\n",
    "\n",
    "for i in range(len(y_test_bulk)):\n",
    "    #model = pickle.load(open(save_model_name, 'rb'))\n",
    "    pred_tree = test_model(model, x_test_bulk[i])\n",
    "    model_evalution(y_test_bulk[i],pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,y = data_loader(file_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y = (X_train, y_train)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pc_values = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaDf = pd.DataFrame(data = pc_values, columns = ['pc 1', 'pc 2'])\n",
    "pcaDf['Target'] = y\n",
    "pcaDf.head()\n",
    "\n",
    "# sns.FacetGrid(pcaDf,hue='Target',height=6).map(plt.scatter,'pc 1','pc 2').add_legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = {\n",
    "            0:'Cook',\n",
    "            1:'Eat',\n",
    "            2:'Phone',\n",
    "            3:'Read',\n",
    "            4:'Watch_TV'\n",
    "        }\n",
    "\n",
    "pcaDf['Target'] = pcaDf[\"Target\"].map(dmap)\n",
    "#pcaDf.head()\n",
    "sns.FacetGrid(pcaDf,hue='Target',height=6).map(plt.scatter,'pc 1','pc 2').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(27, 9))\n",
    "dmap = {\n",
    "            0:'Cook',\n",
    "            1:'Eat',\n",
    "            2:'Phone',\n",
    "            3:'Read',\n",
    "            4:'Watch_TV'\n",
    "        }\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    X,y = data_loader(file_list[i])\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pc_train = pca.fit_transform(X_train)\n",
    "    pcaDf = pd.DataFrame(data = pc_train, columns = ['pc 1', 'pc 2'])\n",
    "    pcaDf['Target'] = y_train\n",
    "    pcaDf['Target'] = pcaDf[\"Target\"].map(dmap)\n",
    "    sns.FacetGrid(pcaDf,hue='Target',height=6).map(plt.scatter,'pc 1','pc 2').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(30, 30))\n",
    "\n",
    "count = 1\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    X,y = data_loader(file_list[i])\n",
    "    pca = PCA(n_components=2)\n",
    "    pc_train = pca.fit_transform(X)\n",
    "    pcaDf = pd.DataFrame(data = pc_train, columns = ['pc 1', 'pc 2'])\n",
    "    pcaDf['Target'] = y\n",
    "    \n",
    "    ax = plt.subplot(6, 2, count)\n",
    "    ax.scatter(X[:,0], y, color='r')\n",
    "    ax.set_xlabel('X 0')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('X vs y')\n",
    "    \n",
    "    ax = plt.subplot(6, 2, count+1)\n",
    "    ax.scatter(X[:,1], y, color='b')\n",
    "    ax.set_xlabel('X 1')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('X vs y')\n",
    "    count+=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pc_values\n",
    "\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [linearly_separable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "         ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "         clf.fit(X_train, y_train)\n",
    "         score = clf.score(X_test, y_test)\n",
    "\n",
    "#         # Plot the decision boundary. For that, we will assign a color to each\n",
    "#         # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "         if hasattr(clf, \"decision_function\"):\n",
    "             Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "         else:\n",
    "             Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "#         # Put the result into a color plot\n",
    "         Z = Z.reshape(xx.shape)\n",
    "         #Z = Z.flatten().reshape(1960,420)\n",
    "         ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "         print(Z.shape)\n",
    "\n",
    "#         # Plot the training points\n",
    "         ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,edgecolors='k')\n",
    "#         # Plot the testing points\n",
    "         ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,edgecolors='k', alpha=0.6)\n",
    "\n",
    "         ax.set_xlim(xx.min(), xx.max())\n",
    "         ax.set_ylim(yy.min(), yy.max())\n",
    "         ax.set_xticks(())\n",
    "         ax.set_yticks(())\n",
    "         if ds_cnt == 0:\n",
    "             ax.set_title(name)\n",
    "         ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                 size=15, horizontalalignment='right')\n",
    "         i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "#     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     AdaBoostClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     QuadraticDiscriminantAnalysis()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig=plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.scatter(X[:, 0], X[:, 1], color='r')\n",
    "# ax.scatter(X[:, 0], X[:, 1], color='b')\n",
    "ax.set_xlabel('Grades Range')\n",
    "ax.set_ylabel('Grades Scored')\n",
    "ax.set_title('scatter plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "count = 0\n",
    "\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        col.scatter(X[:, 0], X[:, 1], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(27, 9))\n",
    "\n",
    "for i in range(1,10):\n",
    "    ax = plt.subplot(3, 3, i)\n",
    "    ax.scatter(X[:, 0], X[:, 1], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
