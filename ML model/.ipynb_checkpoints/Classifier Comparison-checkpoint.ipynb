{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "h = .02  # step size in the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\",\"Decision Tree\", \"Random Forest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_finder(path):\n",
    "\n",
    "    file_list = []\n",
    "\n",
    "    for j in glob(path+\"*.csv\"):\n",
    "        file_list.append(j)\n",
    "\n",
    "    return file_list\n",
    "\n",
    "def read_file(path):\n",
    "\n",
    "    data = pd.read_csv(path, na_values = 'NaN', keep_default_na = False) \n",
    "    return data\n",
    "\n",
    "def pd_to_np(data):\n",
    "    \n",
    "    if type(data) == np.ndarray:\n",
    "      print('Data is already in numpy format!')\n",
    "    else:\n",
    "      data = data.values\n",
    "      #print('Pandas to Numpy done!')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def string_to_index(activity_label):\n",
    "\n",
    "    activity_class =[]\n",
    "    har_class = {\n",
    "                    'Cook':0,\n",
    "                    'Eat':1,\n",
    "                    'Phone':2,\n",
    "                    'Read':3,\n",
    "                    'Watch_TV':4\n",
    "                }\n",
    "    for label in activity_label:\n",
    "        activity_class.append(har_class[label[0]])\n",
    "\n",
    "    return activity_class\n",
    "\n",
    "\n",
    "def data_loader(path, split=0.3):\n",
    "    x = y = []\n",
    "    feature_list = []\n",
    "    \n",
    "    pd_data = read_file(path)\n",
    "\n",
    "    for i in pd_data:\n",
    "        feature_list.append(i)\n",
    "\n",
    "    selectData = pd_data.loc[:, feature_list[:-1]]\n",
    "    activityLabel = pd_data.loc[:, ['activity']]\n",
    "    x = pd_to_np(selectData)\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    y = string_to_index(activityLabel.values)\n",
    "    y = np.asarray(y) \n",
    "    y = y.astype('int32')\n",
    "    return x,y\n",
    "\n",
    "def full_dataset(file_list):\n",
    "\n",
    "    x=y = np.asarray([])\n",
    "    x_temp =  y_temp =  []\n",
    "\n",
    "    for i in range(len(file_list)):\n",
    "        x_temp, y_temp = data_loader(file_list[i])\n",
    "\n",
    "        if i == 0:\n",
    "            x = x_temp\n",
    "            y = y_temp\n",
    "        else:\n",
    "            x = np.concatenate([x, x_temp],axis=0)\n",
    "            y  = np.concatenate([y ,y_temp],axis=0)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Dataset\\\\l1_based_feature.csv']\n",
      "Total files: 1\n"
     ]
    }
   ],
   "source": [
    "file_list = folder_finder(\"../Dataset/\")\n",
    "print(file_list)\n",
    "print('Total files:',len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:747: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:688: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447530, 34)\n",
      "(447530,)\n"
     ]
    }
   ],
   "source": [
    "X,y= full_dataset(file_list)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d62d73835bde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \"\"\"\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[1;32m--> 382\u001b[1;33m                         copy=self.copy)\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;31m# Handle n_components==None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pc_values = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaDf = pd.DataFrame(data = pc_values, columns = ['pc 1', 'pc 2'])\n",
    "pcaDf['Target'] = y\n",
    "pcaDf.head()\n",
    "\n",
    "# sns.FacetGrid(pcaDf,hue='Target',height=6).map(plt.scatter,'pc 1','pc 2').add_legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = {\n",
    "            0:'Cook',\n",
    "            1:'Eat',\n",
    "            2:'Phone',\n",
    "            3:'Read',\n",
    "            4:'Watch_TV'\n",
    "        }\n",
    "\n",
    "pcaDf['Target'] = pcaDf[\"Target\"].map(dmap)\n",
    "#pcaDf.head()\n",
    "sns.FacetGrid(pcaDf,hue='Target',height=6).map(plt.scatter,'pc 1','pc 2').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(27, 9))\n",
    "dmap = {\n",
    "            0:'Cook',\n",
    "            1:'Eat',\n",
    "            2:'Phone',\n",
    "            3:'Read',\n",
    "            4:'Watch_TV'\n",
    "        }\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    X,y = data_loader(file_list[i])\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pc_train = pca.fit_transform(X)\n",
    "    pcaDf = pd.DataFrame(data = pc_train, columns = ['pc 1', 'pc 2'])\n",
    "    pcaDf['Target'] = y\n",
    "    pcaDf['Target'] = pcaDf[\"Target\"].map(dmap)\n",
    "    sns.FacetGrid(pcaDf,hue='Target',height=6).map(plt.scatter,'pc 1','pc 2').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(30, 30))\n",
    "\n",
    "count = 1\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    X,y = data_loader(file_list[i])\n",
    "    pca = PCA(n_components=2)\n",
    "    pc_train = pca.fit_transform(X)\n",
    "    pcaDf = pd.DataFrame(data = pc_train, columns = ['pc 1', 'pc 2'])\n",
    "    pcaDf['Target'] = y\n",
    "    \n",
    "    ax = plt.subplot(6, 2, count)\n",
    "    ax.scatter(X[:,0], y, color='r')\n",
    "    ax.set_xlabel('X 0')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('X vs y')\n",
    "    \n",
    "    ax = plt.subplot(6, 2, count+1)\n",
    "    ax.scatter(X[:,1], y, color='b')\n",
    "    ax.set_xlabel('X 1')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('X vs y')\n",
    "    count+=2\n",
    "    plt.savefig('{:03d}.png'.format(len(file_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pc_values\n",
    "\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [linearly_separable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "         ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "         clf.fit(X_train, y_train)\n",
    "         score = clf.score(X_test, y_test)\n",
    "\n",
    "#         # Plot the decision boundary. For that, we will assign a color to each\n",
    "#         # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "         if hasattr(clf, \"decision_function\"):\n",
    "             Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "         else:\n",
    "             Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "#         # Put the result into a color plot\n",
    "         Z = Z.reshape(xx.shape)\n",
    "         #Z = Z.flatten().reshape(1960,420)\n",
    "         ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "         print(Z.shape)\n",
    "\n",
    "#         # Plot the training points\n",
    "         ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,edgecolors='k')\n",
    "#         # Plot the testing points\n",
    "         ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,edgecolors='k', alpha=0.6)\n",
    "\n",
    "         ax.set_xlim(xx.min(), xx.max())\n",
    "         ax.set_ylim(yy.min(), yy.max())\n",
    "         ax.set_xticks(())\n",
    "         ax.set_yticks(())\n",
    "         if ds_cnt == 0:\n",
    "             ax.set_title(name)\n",
    "         ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                 size=15, horizontalalignment='right')\n",
    "         i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
    "\n",
    "def train_model(model, X_train, y_train):\n",
    "    return model.fit(X_train, y_train)\n",
    "\n",
    "def test_model(model,X_test):\n",
    "\tpred = model.predict(X_test)\n",
    "\treturn pred\n",
    "    \n",
    "def model_init():\n",
    "\n",
    "\tmodel = classifiers[0]\n",
    "\t# model = DecisionTreeClassifier()\n",
    "\treturn model\n",
    "    \n",
    "    \n",
    "def heatmap_cm(confusion_matrix):\n",
    "    sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# For model evolution\n",
    "def model_evalution(y_test, y_pred):\n",
    "     print(\"------------------- Model evaluation ----------------\\n\\n\")\n",
    "     print(\"Confusion Matrix : \\n\",confusion_matrix(y_test, y_pred))\n",
    "     print(\"--------------------------------------------\")\n",
    "     print(\"Accuracy Score : \",accuracy_score(y_test,y_pred))\n",
    "     print(\"Classification Report : \\n\",classification_report(y_test, y_pred))\n",
    "     print(\"--------------------------------------------\")\n",
    "     print(\"\")\n",
    "    \n",
    "    heatmap_cm(pd.DataFrame(confusion_matrix(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
